Что получилось сделать:
1. Разобраться в дааных, как они устроены
2. Сделал отчет(EDA), который основывается на описательных статистиках в описании данных на Kaggle, 
3. Задать несколько простых вопросов, которые могут быть интерсны потенциальным или существующим продавцам на рынке.
4. Работа с Impala, все SQL запросы и вычислительные дейставия делал через нее. 
5. Работа с Plotly, раньше делал все через matplotlib.
6. Pandas использовал только для Plotly

Что не получилось:
1. Много времени ушло на разбор(понимание) данных, до сих пор остаются некоторые вопросы, как же все работает. 
2. Без конкретной задачи над данными, трудно придумать стоящие вопросы самому, поэтому у меня так и не получилось толком
посмотреть на данные в разрезе. 
3. Не успел описать все словами в самом отчете, поэтому буду говорить.

Вопросы:
1. Как научиться быстрее понимать описание данных, быстро принимать решения нужны они тебе или нет?
2. Как быстро понимать что ты соединил правильные таблицы, что они правильно все сгруппированы, то есть сделал то, что нужно.
SQL пока что трудно дается, а в анализе нужно быть увереным, что все сделал правильно.
3. Как происходит работа со Spark в Сбербанке, как храняться данные или как они туда заносятся. 
В RDD можно хранить целую директорию с множеством csv файлов, можно ли как то обращаться к отделым файлам внутри RDD и
работать с ними: join aggregate, как будто у нас имеется DataBase, и Tables в ней?
